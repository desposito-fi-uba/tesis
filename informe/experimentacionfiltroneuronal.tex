\section{Experimentación con el filtro neuronal}

A diferencia del filtro adaptativo, el filtro neuronal necesita una etapa de entrenamiento, con lo que la experimentación se llevó a cabo en dos etapas, la primera de entrenamiento y la segunda de evaluación. 

Nuevamente, antes de poder utilizar el filtro neuronal es necesario realizar una calibración de sus parámetros.

\subsection{Parámetros del filtro}

De las ecuaciones vistas en la sección \ref{sec:tecnicas_filtrado_redes_neuronales} se desprende que es necesario definir los siguientes parámetros:

\begin{itemize}
	\item \textbf{L} Tamaño de la ventana en la STFT.
	\item \textbf{N} Tamaño de la DFT usada para computar la STFT o cantidad de muestras en el dominio de la frecuencia en la STFT.
	\item \textbf{R} Solapamiento de ventanas o tasa de muestreo en el tiempo en la STFT.
	\item \textbf{J} Cantidad de ventanas que conforman la STFT o cantidad de muestras en el dominio del tiempo en la STFT. 
	\item Función de error.
	\item Algoritmo de entrenamiento.
	\item Tamaño de los lotes de entrenamiento.
	\item Estructura de la red neuronal.
\end{itemize}

\subsubsection{L Tamaño de la ventana en la STFT}

El tamaño de la ventana en la STFT afecta distintas características del filtrado:

\begin{itemize}
	\item Relación de compromiso entre la resolución en el dominio de la frecuencia y la resolución en el dominio del tiempo. Recordemos de la sección \ref{sec:espectrograma}, que es preferible tener más resolución en tiempo para poder reconocer los distintos sonidos. Por ende mientras más chica sea el tamaño de la ventana mejor.
	\item La señal de audio filtrada se obtiene de a una ventana a la vez, por lo tanto también es preferible una ventana corta para así tener el menor retraso posible.
	\item A medida que la ventana se hace mas chica, a un mismo poder de cómputo, aumenta el tiempo de procesamiento, ya que se necesitan más pasadas por la red neuronal.
\end{itemize}

Como se observa en la descripción anterior, no hay una forma analítica o directa de establecer el tamaño de la ventana. Un valor que se encontró experimentalmente y que mantiene una relación de compromiso adecuada entre los efectos antes mencionados, es de $\SI{16}{ms}$ o $\SI{32}{ms}$. En el presente trabajo se escogió $\SI{16}{ms}$.

Dado que las señales se encuentran muestreadas a $16 Khz$, esto nos da un largo de ventana igual a:

\begin{equation*}
	L = \SI{16}{ms} \cdot \SI{16}{KHz} = 256
\end{equation*}

\subsubsection{N Cantidad de muestras en el dominio de la frecuencia en la STFT}
\label{sec:cantidad_muestras_frecuencia}

La tasa de muestreo en frecuencia afecta las siguientes características:

\begin{itemize}
	\item A medida que aumentamos la tasa de muestreo en frecuencia, mayor resolución tendremos en ella.
	\item A medida que aumentamos la resolución en frecuencia aumenta el tiempo de cómputo de la FFT y aumenta la cantidad de parámetros de las capas convolucionales de la red neuronal.
	\item Recordemos de la sección \ref{sec:muestreo_en_tiempo_y_frecuencia} que $R \leq L \leq N$, es decir $N \ge 256$.
\end{itemize}

Nuevamente tenemos una relación de compromiso que debemos mantener en el nivel adecuado. Para el presente trabajo se utilizó una FFT de $2^{9} = 512$ puntos. 

\subsubsection{R Solapamiento de ventanas en la STFT}

En la sección \ref{sec:overlapp_and_and} vimos que una elección que permite recuperar la señal de entrada dada la STFT, es utilizar una ventana de Hamming que cumpla la relación $R = L/2$. Dado que en nuestro caso tenemos $L = 256$, obtenemos que $R=128$. Con este valor de $R$ también verificamos la condición necesaria $R \leq L \leq N$, ya que tenemos $128 \leq 256 \leq 512$.

\subsubsection{J Cantidad de muestras en el dominio del tiempo en la STFT}
\label{sec:ventanas_en_espectrogramas}

La cantidad de ventanas a incluir en cada espectrograma, define qué tanta información hacia atrás en el tiempo puede ver la red neuronal, para predecir la ventana de señal de habla filtrada en el tiempo i. 

Al aumentar o disminuir la cantidad de ventanas $J$ tendremos que:

\begin{itemize}
	\item A mayor cantidad de ventanas, más información tendrá la red para predecir y por ende menor el error de predicción. También, más grande es la dimensión del espectrograma y por ende mayor será la cantidad de parámetros de las capas convolucionales de la red neuronal.
	\item A partir de cierto punto difuso no tiene sentido seguir aumentando la cantidad de ventanas, ya que la ventana en el tiempo $i$ deja de estar correlacionada con la ventana $i - J - 1$ y por ende, no hay información útil que la red pueda utilizar en la predicción de la ventana $i$.
\end{itemize}

Experimentalmente se encontró que un número razonable de ventanas es $J=64$.

\subsubsection{Función de error o función de costo}

La función de error define la medida que se busca minimizar durante el entrenamiento de la red. El objetivo de esta función es, permitir ajustar los pesos de la red de tal manera que la medida sea cada vez más chica. En nuestro caso, a la salida de la red tenemos una ventana de la señal filtrada en el dominio de la frecuencia y como valor objetivo tenemos una ventana de la señal de habla sin ruido, también en el dominio de la frecuencia. La función de costo calcula el error entre lo obtenido a la salida de la red y el valor objetivo. 

La función de costo más adecuada para la tarea en cuestión es el error cuadrático medio:

\begin{equation*}
	l = \frac{1}{K} \sum_{i=1}^K (Y(n_i, w_i) - X(n_i, w_i))^2
\end{equation*}

\subsubsection{Algoritmo de entrenamiento}

Para poder entrenar la red neuronal es necesario escoger un optimizador. Los optimizadores se pueden ver como funciones que definen la metodología en la que se van a actualizar los pesos de la red. En el caso de los filtros adaptativos, los distintos tipos de optimizadores utilizados dan lugar a distintos filtros como el LMS, el NLMS, el RLS, etc.

Dos algoritmos, comúnmente utilizados para entrenar redes neuronales, son el método de gradiente estocástico con momento \cite{deep_learning} y el método Adam \cite{adam_optimizer}. Usualmente, el método Adam converge mas rápido gracias al uso de momentos de segundo orden.

Configurar correctamente los hiper-parámetros del optimizador resulta esencial para poder minimizar, tanto el error durante el entrenamiento como el error de generalización.

Para escoger el optimizador adecuado se evaluaron ambos experimentalmente y con Adam se obtuvieron mejores resultados, con respecto al error de generalización y la velocidad de entrenamiento. 

El optimizador Adam depende de tres hiper-parámetros, $\eta$, $\beta_1$ y $\beta_2$. El parámetro $\eta$ define la tasa de aprendizaje y los parámetros $\beta_1$ y $\beta_2$ permiten ajustar las estimaciones del valor medio y la varianza del gradiente de la función de error. Para el presente trabajo los valores utilizados fueron: $lr=1e^{-4}$, $\beta_1=0.9$ y $\beta_2=0.999$.

\subsubsection{Tamaño de los lotes de entrenamiento}

El tamaño de lote de entrenamiento define cuántas muestras deben pasar por la red, antes de calcular el error y ajustar los pesos o parámetros. El rango en el que debe estar el tamaño de lote es entre una muestra y el tamaño del conjunto de entrenamiento completo. En general, en lo referido al entrenamiento de redes neuronales, se suele utilizar el concepto de \emph{mini-batch}, es decir un lote pequeño. 

Idealmente, el tamaño del lote debería ser el conjunto de entrenamiento completo pero esto trae varios problemas.

\begin{itemize}
	\item Se ha demostrado experimentalmente que tamaños de lotes grandes afectan negativamente el error de generalización de la red \cite{on_large_batch_training}.
	\item Un lote grande requiere mucha memoria, la cual suele ser limitada en especial cuando el entrenamiento se lo realiza en una o mas GPUs.
\end{itemize}

Por otro lado, si el tamaño de lote es muy pequeño, el algoritmo de optimización no logra encontrar la dirección de minimización. En estos casos, el gradiente presenta una gran variación muestra a muestra.

Por último, en nuestro caso, debemos también tener en cuenta que una señal de habla no es la muestra que ingresa a la red sino que lo que ingresa a la red son espectrogramas de esa señal de habla. Las señales de habla utilizadas en el presente trabajo tienen en promedio 80 espectrogramas. Es esencial para el entrenamiento adecuado de la red, que antes de cada actualización de los parámetros se haya pasado por la red muestras de distintos audios, de lo contrario el optimizador no logrará encontrar la dirección de minimización. En el presente trabajo, durante el entrenamiento, se utilizó un tamaño de lote de $500$ muestras.

\subsubsection{Estructura de la red neuronal}

En la sección \ref{sec:cantidad_muestras_frecuencia} vimos que el largo de la DFT utilizada para el cómputo de la STFT es de $N=512$. Las señales de habla son señales reales, con lo cual la DFT tendrá simetría conjugada, esto nos lleva a que la información espectral esté duplicada.

Utilizar toda la DFT es innecesario e implicaría un tiempo de cómputo mayor. Debido a la simetría que presenta, toda la información útil de la DFT la podemos encontrar en cada una de las mitades de la simetría. Esto nos lleva a que los espectrogramas que sirven como entrada a la red están conformados por $256$ muestras en el dominio de la frecuencia.

Por otro lado, en la sección \ref{sec:ventanas_en_espectrogramas}, vimos que la cantidad de ventanas a incluir en un espectrograma es de $J=64$. Esto nos lleva a obtener el tamaño de los espectrogramas, el cual es de $256 \times 64$. El tamaño de los espectrogramas resulta esencial para definir la estructura de la red.

Como vimos en la sección \ref{sec:redes_tipo_u}, a medida que los espectrogramas pasan por las distintas capas de la red, el tamaño se irá reduciendo hasta terminar el camino del codificador. Luego, pasan a la parte del decodificador, donde el tamaño se va recuperando capa a capa. En el codificador, a medida que se reduce el tamaño se aumenta la cantidad de filtros presentes en la capa convolucional. En el decodificador, a medida que se aumenta el tamaño, se reduce la cantidad de filtros.

En la figura \ref{fig:ch7_red_estructura} podemos ver cómo se va modificando el tamaño de los espectrogramas a medida que pasan por las distintas capas de la red. La notación es \textbf{Tamaño en frecuencia} $\times$ \textbf{Tamaño en tiempo} $\times$ \textbf{Canales}. Cada canal representa el resultado de la convolución entre la entrada y uno de los filtros de la capa.

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.65]{images/ch7/red_estructura.png}}
	\caption{Estructura del filtro neuronal.}
	\label{fig:ch7_red_estructura}
\end{figure}

\subsection{Entrenamiento}

El entrenamiento de la red consiste en pasar por ella espectrogramas de señales ruidosas, obtener el error respecto de los espectrogramas de las señales sin ruido y modificar los pesos de la red de acuerdo a ese error.

Usualmente, durante el entrenamiento, se le muestra a la red más de una vez cada muestra del conjunto de entrenamiento. Es posible que después de cierto punto, la red empiece a aprenderse por fuerza bruta el conjunto de entrenamiento con lo que el error de generalización comienza a aumentar. Para poder detectar este momento se suele introducir una etapa de validación cada x lotes de entrenamiento, la cual es realizada contra el conjunto de prueba. 

La validación se ejecutó cada 1.000 lotes de entrenamiento y en cada una se utilizó un total de 500 audios elegidos de manera aleatoria del conjunto de prueba. En la etapa de validación del lote 40.000 se observó que el error de generalización aumentó y el aumento fue confirmado en los siguientes lotes, por lo que el entrenamiento fue detenido. Dado que a partir del lote 40.000, el error comenzó a aumentar, el modelo seleccionado fue el obtenido en dicho lote y se descartaron los siguientes. En la figura \ref{fig:ch7_entrenamiento_mse} podemos ver el error cuadrático medio durante el entrenamiento.

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.65]{images/ch7/train_mse.png}}
	\caption{Error cuadrático medio durante el entrenamiento.}
	\label{fig:ch7_entrenamiento_mse}
\end{figure}

\subsection{Resultados}
\label{sec:resultados_filtro_neuronal}

\subsubsection{Medida PESQ}

Al igual que para el caso del filtro adaptativo, se puso a prueba el desempeño del filtro neuronal en el conjunto de pruebas, computando las mismas medidas PESQ para cada uno de los niveles de ruido utilizados. 

En la figura \ref{fig:ch7_pesq_by_snr} podemos ver las medidas \textbf{PESQ - Ruidosa} y \textbf{PESQ - Filtrada} en función del nivel de ruido. Se observa que la máxima mejora media en PESQ se logra para los niveles de ruido; $\SI{-5}{dB}$, $\SI{0}{dB}$ y $\SI{5}{dB}$. Para SNRs mayores, la mejora media en la PESQ disminuye manteniéndose positiva hasta los $\SI{20}{dB}$, donde se vuelve negativa. En la tabla \ref{table:ch7_pesq_by_snr} podemos ver el detalle de los resultados de la figura \ref{fig:ch7_pesq_by_snr}.

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.75]{images/ch7/pesq_by_snr.png}}
	\caption{PESQ en función de la SNR.}
	\label{fig:ch7_pesq_by_snr}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{ |c|c|c|c|c|c|c| } 
		\hline
		SNR [dB] & $-5$ & $0$ & $5$ & $10$ & $15$ & $20$ \\ 
		\hline
		PESQ - Ruidosa & 1.96 & 2.37 & 2.65 & 3.00 & 3.29 & 3.56 \\
		PESQ - Filtrada & 2.16 & 2.58 & 2.85 & 3.12 & 3.35 & 3.50 \\
		\hline
	\end{tabular}
	\caption{PESQ en función de la SNR.}
	\label{table:ch7_pesq_by_snr}
\end{table}

%[2199] Noisy PESQ: 2.799255890755177
%[2199] Filtered PESQ: 2.9217428156276575
%[2199] Noisy STOI: 0.9270031623427075
%[2199] Filtered STOI: 0.9177052034994769


En la figura \ref{fig:ch7_pesq_by_noise_type} podemos ver le medida PESQ en función del tipo de ruido. Las clases de ruido donde el filtro logró el mejor desempeño fue en; \emph{Tipeo}, \emph{Aire Acondicionado}, \emph{Aspiradora} e \emph{Impresora}. Esto se debe a que estas clases de ruido son mas estacionarias y por ende le es mas fácil a la red neuronal poder caracterizarlas.

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.75]{images/ch7/pesq_by_noise_type.png}}
	\caption{PESQ en función del tipo de ruido.}
	\label{fig:ch7_pesq_by_noise_type}
\end{figure} 

% [2199] Noisy PESQ: 2.9368886407946886
% [2199] Filtered PESQ: 2.9214207319374506
% [2199] Noisy STOI: 0.9390761638343121
% [2199] Filtered STOI: 0.920014304694963

En el ruido \emph{Vecinos} y \emph{Conversación} hay personas hablando, cuyas voces se mezclan con la señal de habla principal, dificultándole a la red identificar cuál es la señal a filtrar.

El ruido \emph{Masticar} resulta ser el mas complejo de filtrar y se debe a que es altamente no-estacionario y de corta duración dificultándole a la red caracterizarlo adecuadamente. 

\subsubsection{Medida STOI}

Se puso a prueba el desempeño del filtro neuronal en el conjunto de pruebas, computando las medidas STOI ya definidas, para cada uno de los niveles snr y tipos de ruidos utilizados. 

En la figura \ref{fig:ch7_stoi_by_snr} podemos ver las medidas \textbf{STOI - Ruidosa} y \textbf{STOI - Filtrada} en función del nivel de ruido. Se observa que la máxima mejora media en STOI se logra para los $\SI{-5}{dB}$, y para SNRs mayores, la mejora media en la STOI disminuye. A partir de los $\SI{5}{dB}$ la señal filtrada tiene menor STOI que la ruidosa con lo cual, el filtro no solo no esta logrando mejorar el nivel base de STOI, sino que lo empeora. En el cuadro \ref{table:ch7_stoi_by_snr} podemos ver el detalle de los resultados de la figura \ref{fig:ch7_stoi_by_snr}.

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.8]{images/ch7/stoi_by_snr.png}}
	\caption{STOI en función de la SNR.}
	\label{fig:ch7_stoi_by_snr}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{ |c|c|c|c|c|c|c| } 
		\hline
		SNR [dB] & $-5$ & $0$ & $5$ & $10$ & $15$ & $20$ \\ 
		\hline
		STOI - Ruidosa & 0.80 & 0.89 & 0.93 & 0.97 & 0.97 & 0.99 \\
		STOI - Filtrada & 0.81 & 0.89 & 0.93 & 0.95 & 0.96 & 0.97 \\
		\hline
	\end{tabular}
	\caption{STOI en función de la SNR.}
	\label{table:ch7_stoi_by_snr}
\end{table}


En la figura \ref{fig:ch7_stoi_by_noise_type} podemos ver le medida STOI en función del tipo de ruido. A diferencia de la medida PESQ, la red logró una mejora unicamente para el tipo \emph{Tipeo} e \emph{Impresión}. Para las demás clases, la red no logró mejorar la inteligibilidad de las señales ruidosas.

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.8]{images/ch7/stoi_by_noise_type.png}}
	\caption{STOI en función del tipo de ruido.}
	\label{fig:ch7_stoi_by_noise_type}
\end{figure} 


\subsubsection{Error cuadrático medio y nivel de ruido}

Al igual que para el filtro adaptativo, se obtuvo el error cuadrático medio y el nivel de ruido:

\begin{equation*}
	\text{ECM} = \frac{1}{P-1} \sum_{p=0}^{P} \frac{1}{N_p-1} \sum_{j=0}^{N_p} (\hat{s}_p(n) - s_p(n))^2
\end{equation*}

\begin{equation*}
	\text{Nivel de ruido} = \frac{1}{P-1} \sum_{p=0}^{P} \frac{1}{N_p-1} \sum_{n=0}^{N_p} (n_p(n))^2
\end{equation*}

\begin{figure}
	\centering
	\centerline{\includegraphics[scale=0.8]{images/ch7/ecm_and_noise_level.png}}
	\caption{ECM y Nivel de ruido.}
	\label{fig:ch7_mse_and_noise_level}
\end{figure}

En la figura \ref{fig:ch7_mse_and_noise_level} se muestra el error cuadrático medio y el nivel de ruido como función de la SNR. Podemos observar, que a partir de los $\SI{5}{dB}$ el error cuadrático medio se vuelve comparable con el nivel de ruido. Esto genera que la mejora media en la STOI y PESQ, a partir de los $\SI{5}{dB}$ sean cercanas a 0 o incluso negativas. En estos casos, la señal de habla filtrada tiene un nivel de ruido similar a la señal de habla ruidosa original, pero esta vez inducido por el error de generalización de la red.